{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJua8IcHoQkq",
        "outputId": "9cc9b5a6-be7d-45ed-b63b-abf9cc897af6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip  # ensures that pip is current\n",
        "!git clone https://github.com/google-research/bleurt.git\n",
        "%cd bleurt\n",
        "!pip install .\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ99Jk1UoZpi",
        "outputId": "ca8f323b-659c-42a1-ce05-047c1c88e221"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n",
        "!unzip BLEURT-20.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UJfKIkoocCr",
        "outputId": "aacb7ee6-fe18-4d93-c569-f995f6d0938b"
      },
      "outputs": [],
      "source": [
        "!pip install bert_score\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPjCtQfyowIg"
      },
      "outputs": [],
      "source": [
        "#This is a sample output file. In order to test your submissions you can put outputs in this format\n",
        "\n",
        "import json\n",
        "arr = []\n",
        "m = {\"premise\": \"I left my adult son home for a few days and just came back to a sink full of gross old dishes.\", \"hypothesis\": \"I was gone for only a few days and my considerate adult son just let the sink fill up with dirty dishes, making me feel really happy\",\n",
        "        \"label\": \"Contradiction\", \"explanation\": \"Most people would not consider leaving dirty dishes in the sink for days as a considerate thing to do and so the son's actions cannot make the speaker feel happy.\",\n",
        "        \"predicted_label\": \"Contradiction\",\"model_explanation\": \"Leaving dirty dishes in the sink is a very inconsiderate act and seeing this would make the speaker upset and angry about their son and not happy.\"}\n",
        "\n",
        "arr.append(m)\n",
        "with open(\"outputs.json\",\"w\") as f:\n",
        "  f.write(json.dumps(arr,indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnYGWkINofQ8",
        "outputId": "b7a4d288-0c09-45b9-d860-b9b5b7c80b5f"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "import json\n",
        "import datasets\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "filename = \"outputs.json\"\n",
        "\n",
        "with open(filename) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "count = 0\n",
        "\n",
        "cands1= []\n",
        "refs = []\n",
        "for i in range(len(data)):\n",
        "    if data[i][\"predicted_label\"]!='' and \"explanationscore\" not in data[i]:\n",
        "        cands1.append(data[i][\"model_explanation\"])\n",
        "        refs.append(data[i][\"explanation\"])\n",
        "        print(data[i])\n",
        "\n",
        "P1, R1, F1 = score(cands1, refs, lang=\"en\", model_type='microsoft/deberta-large-mnli',batch_size=1,device=\"cuda:0\")\n",
        "F1 = F1.cpu().detach().numpy().tolist()\n",
        "\n",
        "from bleurt import score\n",
        "scorer = score.BleurtScorer('/content/BLEURT-20')\n",
        "BLEURTscores = scorer.score(references=refs, candidates=cands1, batch_size=1)\n",
        "\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if data[i][\"predicted_label\"]==data[i][\"label\"] and \"explanationscore\" not in data[i]:\n",
        "        cands1.append(data[i][\"model_explanation\"])\n",
        "        refs.append(data[i][\"explanation\"])\n",
        "        data[i][\"explanationscore\"] = int((F1[i]+BLEURTscores[i])*50.0)\n",
        "\n",
        "\n",
        "with open(filename,\"w\") as f:\n",
        "    f.write(json.dumps(data,indent=4)+'\\n')\n",
        "\n",
        "with open(filename) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "count = 0\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "for line in data:\n",
        "    if line[\"label\"]==line[\"predicted_label\"] and line[\"explanationscore\"]>=0:\n",
        "        count = count+1\n",
        "    if line[\"label\"]==line[\"predicted_label\"] and line[\"explanationscore\"]>=50:\n",
        "        count1 = count1+1\n",
        "    if line[\"label\"]==line[\"predicted_label\"] and line[\"explanationscore\"]>=60:\n",
        "        count2 = count2+1\n",
        "\n",
        "print(\"Accuracy@0\",count/len(data))\n",
        "print(\"Accuracy@50\",len(data),count1/len(data))\n",
        "print(\"Accuracy@60\",len(data),count2/len(data))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
